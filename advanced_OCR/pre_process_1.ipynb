{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bdf02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "\n",
    "def preprocess_pdf(file_path):\n",
    "    images = convert_from_path(file_path)\n",
    "    text_pages = []\n",
    "    for image in images:\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        text_pages.append(text)\n",
    "    return '\\n'.join(text_pages)\n",
    "\n",
    "def preprocess_image(file_path):\n",
    "    image = Image.open(file_path)\n",
    "    # Apply preprocessing techniques to enhance image quality (e.g., denoising, thresholding, resizing)\n",
    "    image = image.resize((desired_width, desired_height))\n",
    "    image = image.filter(ImageFilter.Denoise())\n",
    "    image = image.convert('L').point(lambda x: 0 if x < threshold else 255, '1')\n",
    "    text = pytesseract.image_to_string(image)\n",
    "    return text\n",
    "\n",
    "pdf_directory = 'pdf_documents'\n",
    "image_directory = 'image_documents'\n",
    "preprocessed_texts = []\n",
    "\n",
    "# Preprocess PDF files\n",
    "pdf_texts = []\n",
    "for filename in os.listdir(pdf_directory):\n",
    "    if filename.endswith('.pdf'):\n",
    "        file_path = os.path.join(pdf_directory, filename)\n",
    "        preprocessed_text = preprocess_pdf(file_path)\n",
    "        pdf_texts.append(preprocessed_text)\n",
    "\n",
    "# Preprocess image files\n",
    "image_texts = []\n",
    "for filename in os.listdir(image_directory):\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        file_path = os.path.join(image_directory, filename)\n",
    "        preprocessed_text = preprocess_image(file_path)\n",
    "        image_texts.append(preprocessed_text)\n",
    "\n",
    "# Combine PDF and image texts\n",
    "all_texts = pdf_texts + image_texts\n",
    "\n",
    "# Rest of your code for feature extraction, training the model, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd306098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "reading, ieltsfever, com, people, test, academic, www, passage, music, answer, questions, help, https, write, public, information, new, says, sheet, boxes, fb, water, used, ee, use, work, eltsfever, following, years, human\n",
      "\n",
      "Topic 2:\n",
      "banana, island, easter, crocodiles, body, water, paragraph, disease, statues, moai, crocodile, years, varieties, genetic, sigatoka, bananas, fruit, long, crocodilian, edible, crops, resistant, world, year, survive, paragraphs, theories, breeders, plants, stone\n",
      "\n",
      "Topic 3:\n",
      "innovation, fatty, trans, acids, food, passage, com, products, questions, new, answer, people, reading, companies, information, test, ieltsfever, 20, write, www, process, sheet, boxes, biofuels, crops, company, risk, ielts, based, energy\n",
      "\n",
      "Topic 4:\n",
      "world, old, new, ee, seaweeds, 45, ieltsfever, fb, https, oooo, academic, species, oe, helpg, optimism, section, test, animals, ae, water, sea, oo, eltsfever, america, optimists, seaweed, european, people, com, ooo\n",
      "\n",
      "Topic 5:\n",
      "reading, ieltsfever, com, test, questions, www, academic, new, answer, passage, people, write, sheet, boxes, time, power, practice, ielts, help, research, like, world, information, used, use, years, unit, https, 20, effect\n",
      "\n",
      "Topic 6:\n",
      "com, ieltsfever, reading, www, answer, passage, questions, sheet, write, boxes, test, people, help, sun, silk, information, years, 20, words, use, correct, academic, tourism, temperature, following, time, star, choose, new, language\n",
      "\n",
      "Topic 7:\n",
      "com, ieltsfever, reading, www, questions, people, answer, passage, test, new, write, sheet, boxes, world, time, academic, help, information, 20, water, https, correct, century, work, years, following, banana, given, use, 14\n",
      "\n",
      "Topic 8:\n",
      "malaria, travel, italy, people, disease, answer, brain, lie, art, passage, reading, century, rose, questions, write, moai, mary, world, quinine, mosquito, accounts, sheet, information, travelers, snowden, using, new, technology, research, hull\n",
      "\n",
      "Topic 9:\n",
      "students, com, www, teacher, austerity, learning, style, measures, questions, flavour, natural, student, reading, information, passage, government, answer, governments, economy, teaching, process, correct, letter, spending, model, sheet, write, ieltsfever, imf, topsage\n",
      "\n",
      "Topic 10:\n",
      "com, ieltsfever, nobel, reading, academic, people, test, ee, fb, birds, eltsfever, https, ingenuity, power, www, 44, alfred, company, help, shower, carbolic, flu, answer, style, section, smoke, ball, ozkleen, information, styles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kranthi/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer to convert text documents into a matrix of token counts\n",
    "vectorizer = CountVectorizer(stop_words='english', lowercase=True, decode_error='ignore')\n",
    "X = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "# Define the number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Apply Latent Dirichlet Allocation (LDA)\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Get the topic-word matrix\n",
    "topic_word_matrix = lda.components_\n",
    "\n",
    "# Get the most important words for each topic\n",
    "num_top_words = 30\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "topics = []\n",
    "for topic_idx, topic in enumerate(topic_word_matrix):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]\n",
    "    topics.append(top_words)\n",
    "\n",
    "# Print the topics and their associated keywords\n",
    "for topic_idx, top_words in enumerate(topics):\n",
    "    print(f\"Topic {topic_idx + 1}:\")\n",
    "    print(\", \".join(top_words))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ed786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
